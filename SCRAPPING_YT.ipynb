{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8FyoLRfwHEp"
      },
      "source": [
        "# SCRAPPING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlaF0hnxwFz9",
        "outputId": "e47b1855-2e9f-4685-d293-0e16d8d053bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (2.183.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "pip install google-api-python-client pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fGwBV9YwR96",
        "outputId": "1714a54a-fa2f-4fd8-a07c-5186e049cda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memulai scraping komentar untuk Video ID: sPyb2e0l5Vw\n",
            "  Sudah mendapatkan 100 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 200 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 300 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 400 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 500 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 600 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 700 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 800 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 900 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1000 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1100 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1200 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1300 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1400 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1500 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1600 komentar. Melanjutkan ke halaman berikutnya...\n",
            "  Sudah mendapatkan 1700 komentar. Melanjutkan ke halaman berikutnya...\n",
            "Scraping selesai. Total komentar: 1788\n",
            "Data telah berhasil disimpan ke file: SEDIH4_comments.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURASI ---\n",
        "API_KEY = \"API_KEY\"\n",
        "VIDEO_ID = \"sPyb2e0l5Vw\"\n",
        "\n",
        "\n",
        "def get_comments(video_id, api_key):\n",
        "    # Inisialisasi layanan YouTube API\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "    comments_list = []\n",
        "    next_page_token = None\n",
        "\n",
        "    print(f\"Memulai scraping komentar untuk Video ID: {video_id}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Panggil API untuk mengambil thread komentar\n",
        "            request = youtube.commentThreads().list(\n",
        "                part=\"snippet\",\n",
        "                videoId=video_id,\n",
        "                maxResults=100,  # Maksimum yang diizinkan per permintaan\n",
        "                pageToken=next_page_token\n",
        "            )\n",
        "\n",
        "            response = request.execute()\n",
        "\n",
        "            for item in response['items']:\n",
        "                comment = item['snippet']['topLevelComment']['snippet']\n",
        "\n",
        "                # Ekstrak data yang Anda inginkan\n",
        "                comments_list.append({\n",
        "                    'author': comment['authorDisplayName'],\n",
        "                    'comment_text': comment['textDisplay'],\n",
        "                    'published_at': comment['publishedAt'],\n",
        "                    'like_count': comment['likeCount']\n",
        "                })\n",
        "\n",
        "            # Periksa apakah ada halaman berikutnya\n",
        "            next_page_token = response.get('nextPageToken')\n",
        "\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "            print(f\"  Sudah mendapatkan {len(comments_list)} komentar. Melanjutkan ke halaman berikutnya...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Terjadi kesalahan: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"Scraping selesai. Total komentar: {len(comments_list)}\")\n",
        "    return comments_list\n",
        "\n",
        "# Jalankan fungsi\n",
        "comments_data = get_comments(VIDEO_ID, API_KEY)\n",
        "\n",
        "# Konversi ke DataFrame Pandas\n",
        "df = pd.DataFrame(comments_data)\n",
        "\n",
        "# Simpan ke file CSV\n",
        "output_filename = f\"SEDIH4_comments.csv\"\n",
        "df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Data telah berhasil disimpan ke file: {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7KFumUfxjxk"
      },
      "source": [
        "MARAH: 34889+128\n",
        "SEDIH: 354+414+879+...+1788\n",
        "BAHAGIA: 27780\n",
        "TERKEJUT: 1583+134\n",
        "TAKUT: 140+3016+2207\n",
        "CINTA: 407+8+100+24+920+1678"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1U0lV76m7t"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBJ4s9yD52rk",
        "outputId": "fe8d2f7d-d4a8-4e96-f781-a92f45b2005c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memulai proses penggabungan file komentar emosi...\n",
            "\n",
            "--- Ringkasan Komentar Emosi ---\n",
            "✅ BERHASIL menggabungkan 18 file menjadi: DATA_KOMENTAR_EMOSI_GABUNGAN.csv\n",
            "Total baris data: 76751\n",
            "\n",
            "⚠️ PERHATIAN: File berikut tidak ditemukan dan dilewati:\n",
            "- CINTA2_comments.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "files_to_merge = [\n",
        "    \"BAHAGIA_comments.csv\", \"CINTA1_comments.csv\", \"CINTA_comments.csv\",\n",
        "    \"CINTA2_comments.csv\", \"CINTA3_comments.csv\", \"CINTA4_comments.csv\",\n",
        "    \"CINTA5_comments.csv\", \"MARAH_comments.csv\", \"MARAH1_comments.csv\", \"SEDIH_comments.csv\",\n",
        "    \"SEDIH1_comments.csv\", \"SEDIH2_comments\", \"SEDIH3_comments.csv\", \"SEDIH4_comments.csv\",\n",
        "    \"TAKUT_comments.csv\", \"TAKUT1_comments.csv\", \"TAKUT2_comments.csv\", \"TERKEJUT_comments.csv\",\n",
        "    \"TERKEJUT1_comments\"\n",
        "]\n",
        "\n",
        "# Siapkan list kosong untuk menampung semua DataFrame\n",
        "all_dataframes = []\n",
        "missing_files = []\n",
        "read_success_count = 0\n",
        "\n",
        "print(\"Memulai proses penggabungan file komentar emosi...\")\n",
        "\n",
        "# Looping untuk membaca dan memproses setiap file\n",
        "for file in files_to_merge:\n",
        "    # Beberapa file di daftar Anda tidak memiliki ekstensi .csv (misalnya 'SEDIH2_comments')\n",
        "    # Kita tambahkan penanganan untuk kasus ini.\n",
        "    if not file.lower().endswith('.csv'):\n",
        "        # Coba tambahkan .csv jika belum ada\n",
        "        file_name_with_ext = file + \".csv\"\n",
        "    else:\n",
        "        file_name_with_ext = file\n",
        "\n",
        "    try:\n",
        "        # Baca file CSV\n",
        "        df = pd.read_csv(file_name_with_ext)\n",
        "        all_dataframes.append(df)\n",
        "        read_success_count += 1\n",
        "        # print(f\"  [BERHASIL] Memuat: {file_name_with_ext}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Jika file tidak ditemukan, catat dan lewati\n",
        "        missing_files.append(file_name_with_ext)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"  [PERINGATAN] File {file_name_with_ext} kosong. Melewati.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] Gagal memuat {file_name_with_ext}: {e}\")\n",
        "\n",
        "# Gabungkan semua DataFrame dalam list menjadi satu DataFrame besar\n",
        "if all_dataframes:\n",
        "    final_combined_df_emosi = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "    # Simpan hasil gabungan ke file baru\n",
        "    output_filename_emosi = 'DATA_KOMENTAR_EMOSI_GABUNGAN.csv'\n",
        "    final_combined_df_emosi.to_csv(output_filename_emosi, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n--- Ringkasan Komentar Emosi ---\")\n",
        "    print(f\"✅ BERHASIL menggabungkan {read_success_count} file menjadi: {output_filename_emosi}\")\n",
        "    print(f\"Total baris data: {len(final_combined_df_emosi)}\")\n",
        "    if missing_files:\n",
        "        print(\"\\n⚠️ PERHATIAN: File berikut tidak ditemukan dan dilewati:\")\n",
        "        for missing in missing_files:\n",
        "            print(f\"- {missing}\")\n",
        "else:\n",
        "    print(\"\\n❌ GAGAL: Tidak ada file komentar emosi yang berhasil dimuat.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqfVfjj7wYWF",
        "outputId": "07554826-386e-4b5e-9bb1-4624884f0af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berhasil memuat file: DATA_KOMENTAR_EMOSI_GABUNGAN.csv\n",
            "\n",
            "[LANGKAH 1/3] Menerapkan Case Folding...\n",
            "[LANGKAH 2/3] Menerapkan fungsi cleaning kustom...\n",
            "[LANGKAH 3/3] Menyimpan data yang sudah dibersihkan...\n",
            "\n",
            "--- RINGKASAN PROSES ---\n",
            "Proses Case Folding dan Cleaning Teks Selesai!\n",
            "Data disimpan ke file baru: DATA_KOMENTAR_EMOSI_CLEANED.csv\n",
            "\n",
            "Contoh Perubahan Data:\n",
            "Kolom asli ('TEXT'):\n",
            "0                                       sy fan om dedy\n",
            "1         semoga jirayut sama halda jodoh yang sakinah\n",
            "2                                       ini lucu tau😂😂\n",
            "3      disaat halda dah sama sadam, aku masih disini 😢\n",
            "4    itu cewek berisik banget, sok asik tapi garing...\n",
            "Name: comment_text, dtype: object\n",
            "\n",
            "Kolom hasil cleaning ('TEXT_CLEANED'):\n",
            "0                                       sy fan om dedy\n",
            "1         semoga jirayut sama halda jodoh yang sakinah\n",
            "2                                         ini lucu tau\n",
            "3         disaat halda dah sama sadam aku masih disini\n",
            "4    itu cewek berisik banget sok asik tapi garing ...\n",
            "Name: TEXT_CLEANED, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 1. Tentukan nama file input dan output\n",
        "file_input = 'DATA_KOMENTAR_EMOSI_GABUNGAN.csv'\n",
        "file_output = 'DATA_KOMENTAR_EMOSI_CLEANED.csv'\n",
        "\n",
        "# 2. Baca file CSV\n",
        "try:\n",
        "    df = pd.read_csv(file_input)\n",
        "    print(f\"Berhasil memuat file: {file_input}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File '{file_input}' tidak ditemukan. Pastikan file berada di direktori yang sama.\")\n",
        "    exit()\n",
        "\n",
        "# Kolom yang akan diproses\n",
        "kolom_teks = 'comment_text'\n",
        "\n",
        "# --- 3. Case Folding (huruf kecil semua) ---\n",
        "print(\"\\n[LANGKAH 1/3] Menerapkan Case Folding...\")\n",
        "# Pastikan kolom adalah string sebelum di-lowercase\n",
        "df[kolom_teks] = df[kolom_teks].astype(str).str.lower()\n",
        "\n",
        "# --- 4. Fungsi untuk preprocessing bertahap ---\n",
        "def clean_text(text):\n",
        "    \"\"\"Fungsi untuk membersihkan teks dari URL, tag HTML, simbol, dan spasi berlebih.\"\"\"\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)    # hapus URL\n",
        "    text = re.sub(r'<.*?>', '', text)                      # hapus HTML tag\n",
        "    # Hapus angka & simbol, hanya menyisakan huruf (a-z) dan spasi\n",
        "    # PERHATIAN: Ini juga akan menghapus karakter non-ASCII seperti emoji atau karakter bahasa lain.\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()               # hapus spasi berlebih\n",
        "    return text\n",
        "\n",
        "# --- 5. Terapkan Fungsi Cleaning ke Kolom Teks ---\n",
        "print(\"[LANGKAH 2/3] Menerapkan fungsi cleaning kustom...\")\n",
        "# Buat kolom baru untuk menyimpan hasil cleaning, agar kolom asli tetap ada (opsional, tapi disarankan)\n",
        "df['TEXT_CLEANED'] = df[kolom_teks].apply(clean_text)\n",
        "\n",
        "# --- 6. Simpan Data Hasil Cleaning ---\n",
        "print(\"[LANGKAH 3/3] Menyimpan data yang sudah dibersihkan...\")\n",
        "df.to_csv(file_output, index=False, encoding='utf-8')\n",
        "\n",
        "print(\"\\n--- RINGKASAN PROSES ---\")\n",
        "print(f\"Proses Case Folding dan Cleaning Teks Selesai!\")\n",
        "print(f\"Data disimpan ke file baru: {file_output}\")\n",
        "print(\"\\nContoh Perubahan Data:\")\n",
        "print(\"Kolom asli ('TEXT'):\")\n",
        "print(df[kolom_teks].head())\n",
        "print(\"\\nKolom hasil cleaning ('TEXT_CLEANED'):\")\n",
        "print(df['TEXT_CLEANED'].head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
